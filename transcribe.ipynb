{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local transcription using Whisper\n",
    "The following setup can be used to locally transcribe interviews without sending potentially sensitive userdata to an external service. It relies on separately recorded audios for the interviewer and participant to make speaker detection easy.\n",
    "\n",
    "## Prerequesites\n",
    "A local setup for [Jupyter Notebooks](https://jupyter.org/).\n",
    "\n",
    "[ffmpeg](https://ffmpeg.org/) must be installed in the command line.\n",
    "\n",
    "[Whisper](https://github.com/openai/whisper) only works with Python 3.9 or lower due to a dependency to PyTorch.\n",
    "\n",
    "## Input data\n",
    "This setup relies on separate audio files for interviewer and participant. To generate those it is easiest to use [Zoom](https://zoom.us/) and select \"Record a separate audio file for each participant\" in Settings, Recording.\n",
    "\n",
    "When not speaking, microphones should be muted. If possible, rely on video calls and nodding to encourage participants to continue to talk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Whisper and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /private/var/folders/19/c8kd6vzj0z957d7vy39126t40000gn/T/pip-req-build-_gyev3rl\n",
      "  Running command git clone -q https://github.com/openai/whisper.git /private/var/folders/19/c8kd6vzj0z957d7vy39126t40000gn/T/pip-req-build-_gyev3rl\n",
      "  Resolved https://github.com/openai/whisper.git to commit c09a7ae299c4c34c5839a76380ae407e7d785914\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numba in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (0.54.1)\n",
      "Requirement already satisfied: tiktoken==0.3.1 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (0.3.1)\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (0.2.0)\n",
      "Requirement already satisfied: numpy in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (1.20.3)\n",
      "Requirement already satisfied: more-itertools in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (8.10.0)\n",
      "Requirement already satisfied: torch in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from openai-whisper==20230314) (4.62.3)\n",
      "Requirement already satisfied: future in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.26.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2023.3.23)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.2)\n",
      "Requirement already satisfied: setuptools in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from numba->openai-whisper==20230314) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from numba->openai-whisper==20230314) (0.37.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages (from torch->openai-whisper==20230314) (3.10.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables\n",
    "Configure an interviewer and participant name here.\n",
    "\n",
    "The audio files will be expected in the `data/<participantName>/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import json\n",
    "import os\n",
    "\n",
    "interviewerName = 'Interviewer'\n",
    "participantName = 'P1'\n",
    "\n",
    "participantAudio = 'audioP1.m4a'\n",
    "interviewerAudio = 'audioInterviewer.m4a'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "The following code creates an output directory for intermediate data and loads the whisper model. \n",
    "\n",
    "A full list of available models can be found [here](https://github.com/openai/whisper#available-models-and-languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'out/{participantName}/'):\n",
    "    os.mkdir(f'out/{participantName}/')\n",
    "\n",
    "model = whisper.load_model('medium.en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe interview audios using Whisper\n",
    "Transcribe both the interviewee and interviewer audio separately using whisper. Transcription with the `medium.en` model roughly takes 50% of the original interview time.\n",
    "\n",
    "Intermediate data of the transcription is saved as JSON in `out/<participantName>/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 97%|█████████▋| 14208/14649 [02:46<00:05, 85.26frames/s] \n"
     ]
    }
   ],
   "source": [
    "interviewee = model.transcribe(audio=f'data/{participantName}/{participantAudio}', verbose=False)\n",
    "\n",
    "with open(f'out/{participantName}/{participantName}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(interviewee, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pheltweg/opt/anaconda3/lib/python3.9/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 14649/14649 [01:33<00:00, 156.63frames/s]\n"
     ]
    }
   ],
   "source": [
    "interviewer = model.transcribe(audio=f'data/{participantName}/{interviewerAudio}', verbose=False)\n",
    "\n",
    "with open(f'out/{participantName}/{interviewerName}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(interviewer, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up and combine segments\n",
    "Segments from both audios are combined into one JSON file. If multiple segments by the same speaker follow each other, they are combined in one larger segment and all original ids preserved. This ensures that a clear data-trail exists from original audio, to individual transcription, to combined segments.\n",
    "\n",
    "The combined segments are saved as `out/<participantName>/combined.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'out/{participantName}/{interviewerName}.json', 'r') as inputInterviewer:\n",
    "    interviewer = json.load(inputInterviewer)\n",
    "with open(f'out/{participantName}/{participantName}.json', 'r') as inputInterviewee:\n",
    "    interviewee = json.load(inputInterviewee)\n",
    "\n",
    "combinedSegments = []\n",
    "\n",
    "for segment in interviewer['segments']:\n",
    "    combinedSegments.append({\n",
    "        'speaker': interviewerName,\n",
    "        'text': segment['text'].strip(' '),\n",
    "        'original_ids': [segment['id']],\n",
    "        'start': segment['start'],\n",
    "        'end': segment['end'],\n",
    "    })\n",
    "\n",
    "for segment in interviewee['segments']:\n",
    "    combinedSegments.append({\n",
    "        'speaker': participantName,\n",
    "        'text': segment['text'].strip(' '),\n",
    "        'original_ids': [segment['id']],\n",
    "        'start': segment['start'],\n",
    "        'end': segment['end'],\n",
    "    })\n",
    "\n",
    "combinedSegments = sorted(combinedSegments, key=lambda d: d['start'])\n",
    "\n",
    "outputSegments = []\n",
    "for segment in combinedSegments:\n",
    "    # Whisper seems to sometimes just add a . as text if it can not detect speech\n",
    "    if (segment['text'] == '.'):\n",
    "        continue\n",
    "\n",
    "    if (len(outputSegments) == 0 or segment['speaker'] != outputSegments[-1]['speaker']):\n",
    "        outputSegments.append(segment)\n",
    "    else:\n",
    "        outputSegments[-1]['original_ids'].append(segment['original_ids'][0])\n",
    "        outputSegments[-1]['end'] = segment['end']\n",
    "        outputSegments[-1]['text'] += ' ' + segment['text']\n",
    "\n",
    "with open(f'out/{participantName}/combined.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(outputSegments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output\n",
    "Finally, the combined segments are rendered as markdown file in `out/<participantName>.md`, including timestamps. Any other output rendering could be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = f'# Interview {participantName}\\n\\n'\n",
    "\n",
    "for segment in outputSegments:\n",
    "    startMin, startSec = divmod(segment['start'], 60)\n",
    "    endMin, endSec = divmod(segment['end'], 60)\n",
    "    output += f'## [{segment[\"speaker\"]}] ({startMin:02.0f}:{startSec:02.0f} - {endMin:02.0f}:{endSec:02.0f})\\n'\n",
    "    output += f'{segment[\"text\"]}\\n\\n'\n",
    "\n",
    "with open(f'out/{participantName}.md', 'w') as outputFile:\n",
    "    print(output, file=outputFile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual cleaning\n",
    "After creating the output markdown file, copy it to `cleaned` and manually fix any mistakes that occured during transcription. Zoom also provides a combined audio file of the whole interview that can be used to compare the transcription with the actual audio.\n",
    "\n",
    "A useful tool is [VLC](https://www.videolan.org/vlc/) with [global hotkeys](https://wiki.videolan.org/VLC_HowTo/Global_hotkeys/) to jump back/forwards 10 seconds and change replay speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting as PDF\n",
    "Markdown files can be converted to PDF using [pandoc](https://pandoc.org/). \n",
    "\n",
    "An example command that converts `cleaned/P1.md` to `P1.pdf` with 0.5 inch margins is: `pandoc -V geometry:margin=0.5in -o P1.pdf cleaned/P1.md`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fc67d4bd6551940b58ea4744616426ed6eba2917041d94fcc3a3839b71dc9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
