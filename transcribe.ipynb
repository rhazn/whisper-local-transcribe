{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local transcription using Whisper\n",
    "The following setup can be used to locally transcribe interviews without sending potentially sensitive userdata to an external service. It relies on separately recorded audios for the interviewer and participant to make speaker detection easy.\n",
    "\n",
    "## Prerequesites\n",
    "A local setup for [Jupyter Notebooks](https://jupyter.org/).\n",
    "\n",
    "[ffmpeg](https://ffmpeg.org/) must be installed in the command line.\n",
    "\n",
    "[Whisper](https://github.com/openai/whisper) only works with Python 3.9 or lower due to a dependency to PyTorch.\n",
    "\n",
    "## Input data\n",
    "This setup relies on separate audio files for interviewer and participant. To generate those it is easiest to use [Zoom](https://zoom.us/) and select \"Record a separate audio file for each participant\" in Settings, Recording.\n",
    "\n",
    "When not speaking, microphones should be muted. If possible, rely on video calls and nodding to encourage participants to continue to talk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Whisper and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables\n",
    "Configure an interviewer and participant name here.\n",
    "\n",
    "The audio files will be expected in the `data/<participantName>/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import json\n",
    "import os\n",
    "\n",
    "interviewerName = 'Interviewer'\n",
    "participantName = 'P1'\n",
    "\n",
    "participantAudio = 'audioP1.m4a'\n",
    "interviewerAudio = 'audioInterviewer.m4a'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "The following code creates an output directory for intermediate data and loads the whisper model. \n",
    "\n",
    "A full list of available models can be found [here](https://github.com/openai/whisper#available-models-and-languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'out/{participantName}/'):\n",
    "    os.mkdir(f'out/{participantName}/')\n",
    "\n",
    "model = whisper.load_model('medium.en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe interview audios using Whisper\n",
    "Transcribe both the interviewee and interviewer audio separately using whisper. Transcription with the `medium.en` model roughly takes 50% of the original interview time.\n",
    "\n",
    "Intermediate data of the transcription is saved as JSON in `out/<participantName>/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewee = model.transcribe(audio=f'data/{participantName}/{participantAudio}', verbose=False)\n",
    "\n",
    "with open(f'out/{participantName}/{participantName}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(interviewee, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewer = model.transcribe(audio=f'data/{participantName}/{interviewerAudio}', verbose=False)\n",
    "\n",
    "with open(f'out/{participantName}/{interviewerName}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(interviewer, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up and combine segments\n",
    "Segments from both audios are combined into one JSON file. If multiple segments by the same speaker follow each other, they are combined in one larger segment and all original ids preserved. This ensures that a clear data-trail exists from original audio, to individual transcription, to combined segments.\n",
    "\n",
    "The combined segments are saved as `out/<participantName>/combined.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'out/{participantName}/{interviewerName}.json', 'r') as inputInterviewer:\n",
    "    interviewer = json.load(inputInterviewer)\n",
    "with open(f'out/{participantName}/{participantName}.json', 'r') as inputInterviewee:\n",
    "    interviewee = json.load(inputInterviewee)\n",
    "\n",
    "combinedSegments = []\n",
    "\n",
    "for segment in interviewer['segments']:\n",
    "    combinedSegments.append({\n",
    "        'speaker': interviewerName,\n",
    "        'text': segment['text'].strip(' '),\n",
    "        'original_ids': [segment['id']],\n",
    "        'start': segment['start'],\n",
    "        'end': segment['end'],\n",
    "    })\n",
    "\n",
    "for segment in interviewee['segments']:\n",
    "    combinedSegments.append({\n",
    "        'speaker': participantName,\n",
    "        'text': segment['text'].strip(' '),\n",
    "        'original_ids': [segment['id']],\n",
    "        'start': segment['start'],\n",
    "        'end': segment['end'],\n",
    "    })\n",
    "\n",
    "combinedSegments = sorted(combinedSegments, key=lambda d: d['start'])\n",
    "\n",
    "outputSegments = []\n",
    "for segment in combinedSegments:\n",
    "    # Whisper seems to sometimes just add a . as text if it can not detect speech\n",
    "    if (segment['text'] == '.'):\n",
    "        continue\n",
    "\n",
    "    if (len(outputSegments) == 0 or segment['speaker'] != outputSegments[-1]['speaker']):\n",
    "        outputSegments.append(segment)\n",
    "    else:\n",
    "        outputSegments[-1]['original_ids'].append(segment['original_ids'][0])\n",
    "        outputSegments[-1]['end'] = segment['end']\n",
    "        outputSegments[-1]['text'] += ' ' + segment['text']\n",
    "\n",
    "with open(f'out/{participantName}/combined.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(outputSegments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output\n",
    "Finally, the combined segments are rendered as markdown file in `out/<participantName>.md`, including timestamps. Any other output rendering could be added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = f'# Interview {participantName}\\n\\n'\n",
    "\n",
    "for segment in outputSegments:\n",
    "    startMin, startSec = divmod(segment['start'], 60)\n",
    "    endMin, endSec = divmod(segment['end'], 60)\n",
    "    output += f'## [{segment[\"speaker\"]}] ({startMin:02.0f}:{startSec:02.0f} - {endMin:02.0f}:{endSec:02.0f})\\n'\n",
    "    output += f'{segment[\"text\"]}\\n\\n'\n",
    "\n",
    "with open(f'out/{participantName}.md', 'w') as outputFile:\n",
    "    print(output, file=outputFile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual cleaning\n",
    "After creating the output markdown file, copy it to `cleaned` and manually fix any mistakes that occured during transcription. Zoom also provides a combined audio file of the whole interview that can be used to compare the transcription with the actual audio.\n",
    "\n",
    "A useful tool is [VLC](https://www.videolan.org/vlc/) with [global hotkeys](https://wiki.videolan.org/VLC_HowTo/Global_hotkeys/) to jump back/forwards 10 seconds and change replay speed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting as PDF\n",
    "Markdown files can be converted to PDF using [pandoc](https://pandoc.org/). \n",
    "\n",
    "An example command that converts `cleaned/P1.md` to `P1.pdf` with 0.5 inch margins is: `pandoc -V geometry:margin=0.5in -o P1.pdf cleaned/P1.md`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fc67d4bd6551940b58ea4744616426ed6eba2917041d94fcc3a3839b71dc9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
